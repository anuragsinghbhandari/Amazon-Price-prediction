{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce04db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c72a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3c97881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing text...\n",
      "TF-IDF shape: (75000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing text...\")\n",
    "\n",
    "tfidf = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    lowercase=True,\n",
    "    sublinear_tf=True,\n",
    "    max_features=10000,\n",
    "    ngram_range=(1,2)\n",
    ")\n",
    "\n",
    "X = tfidf.fit_transform(df['catalog_content'])\n",
    "# X_valid_vec = tfidf.transform(X_valid)\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cec3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached text embeddings\n",
      "Text embeddings shape: (75000, 384)\n"
     ]
    }
   ],
   "source": [
    "text_embeddings_file = \"text_embeddings.npy\"\n",
    "\n",
    "# Check if cached embeddings exist\n",
    "try:\n",
    "    text_embeddings = np.load(text_embeddings_file)\n",
    "    print(\"Loaded cached text embeddings\")\n",
    "except:\n",
    "    text_model = SentenceTransformer('all-MiniLM-L6-v2')  # small & fast\n",
    "    print(\"Encoding text with BERT...\")\n",
    "    text_embeddings = text_model.encode(df['catalog_content'].tolist(), show_progress_bar=True)\n",
    "    np.save(text_embeddings_file, text_embeddings)\n",
    "    print(\"Text embeddings saved\")\n",
    "\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7568d2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       numeric_value unit_extracted  standardized_value standardized_unit  \\\n",
      "0              72.00          Fl Oz           2129.2920                mL   \n",
      "1              32.00          Ounce            907.2000                 g   \n",
      "2              11.40          Ounce            323.1900                 g   \n",
      "3              11.25          Ounce            318.9375                 g   \n",
      "4              12.00          Count             12.0000             count   \n",
      "...              ...            ...                 ...               ...   \n",
      "74995          12.00          Ounce            340.2000                 g   \n",
      "74996         100.00          count            100.0000             count   \n",
      "74997          80.00          Ounce           2268.0000                 g   \n",
      "74998          16.00          Count             16.0000             count   \n",
      "74999           2.47          Ounce             70.0245                 g   \n",
      "\n",
      "      unit_type  \n",
      "0        volume  \n",
      "1        weight  \n",
      "2        weight  \n",
      "3        weight  \n",
      "4         count  \n",
      "...         ...  \n",
      "74995    weight  \n",
      "74996     count  \n",
      "74997    weight  \n",
      "74998     count  \n",
      "74999    weight  \n",
      "\n",
      "[75000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: extract numeric + unit\n",
    "def extract_value_and_unit(text):\n",
    "    value_match = re.search(r\"Value:\\s*([\\d\\.]+)\", text)\n",
    "    value = float(value_match.group(1)) if value_match else None\n",
    "    \n",
    "    unit_match = re.search(r\"Unit:\\s*([A-Za-z ]+)\", text)\n",
    "    unit = unit_match.group(1).strip() if unit_match else None\n",
    "    \n",
    "    return pd.Series([value, unit])\n",
    "\n",
    "df[[\"numeric_value\", \"unit_extracted\"]] = df[\"catalog_content\"].apply(extract_value_and_unit)\n",
    "\n",
    "# Step 2: normalize units\n",
    "def normalize_units(value, unit):\n",
    "    if pd.isna(unit) or pd.isna(value):\n",
    "        return pd.Series([value, unit, \"unknown\"])\n",
    "    \n",
    "    u = unit.strip().lower()\n",
    "    \n",
    "    # Weight conversions to grams\n",
    "    if u in [\"ounce\", \"oz\", \"ounces\"]:\n",
    "        return pd.Series([value * 28.35, \"g\", \"weight\"])\n",
    "    elif u in [\"pound\", \"lb\", \"lbs\"]:\n",
    "        return pd.Series([value * 453.592, \"g\", \"weight\"])\n",
    "    \n",
    "    # Volume conversions to milliliters\n",
    "    elif u in [\"fl oz\", \"floz\", \"fluid ounce\", \"fluid ounces\", \"Fl Ounce\"]:\n",
    "        return pd.Series([value * 29.5735, \"mL\", \"volume\"])\n",
    "    elif u in [\"liter\", \"litre\", \"l\",\"ltr\"]:\n",
    "        return pd.Series([value * 1000, \"mL\", \"volume\"])\n",
    "    \n",
    "    # Count-based (no conversion)\n",
    "    elif u in [\"count\", \"pack\", \"pcs\", \"piece\", \"pieces\", \"PACK\", \"can\", \"Carton\", \"Tea bags\"]:\n",
    "        return pd.Series([value, \"count\", \"count\"])\n",
    "    \n",
    "    # Unknown\n",
    "    else:\n",
    "        return pd.Series([value, unit, \"unknown\"])\n",
    "\n",
    "df[[\"standardized_value\", \"standardized_unit\", \"unit_type\"]] = df.apply(\n",
    "    lambda x: normalize_units(x[\"numeric_value\"], x[\"unit_extracted\"]), axis=1\n",
    ")\n",
    "\n",
    "print(df[[\"numeric_value\", \"unit_extracted\", \"standardized_value\", \"standardized_unit\", \"unit_type\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "071a2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"unit_type\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))\n",
    "\n",
    "new_feature = df[['standardized_value','unit_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51ca9f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced TF-IDF shape: (75000, 256)\n",
      "Final combined feature shape: (75000, 642)\n"
     ]
    }
   ],
   "source": [
    "# Suppose X_tfidf is your TF-IDF matrix: (n_samples, 10000)\n",
    "svd = TruncatedSVD(n_components=256, random_state=42)\n",
    "X_tfidf_reduced = svd.fit_transform(X)\n",
    "print(\"Reduced TF-IDF shape:\", X_tfidf_reduced.shape)\n",
    "\n",
    "# Scale TF-IDF reduced\n",
    "scaler_tfidf = StandardScaler()\n",
    "X_tfidf_scaled = scaler_tfidf.fit_transform(X_tfidf_reduced)\n",
    "\n",
    "# Scale BERT embeddings (dense)\n",
    "scaler_bert = StandardScaler()\n",
    "X_bert_scaled = scaler_bert.fit_transform(text_embeddings)\n",
    "\n",
    "\n",
    "# Combine all features\n",
    "X_combined = np.hstack([X_tfidf_reduced, text_embeddings, new_feature])\n",
    "\n",
    "print(\"Final combined feature shape:\", X_combined.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d026b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_combined, df['price'],\n",
    "    test_size=0.1, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "839e29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log1p(y_train)\n",
    "y_valid_log = np.log1p(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb91bb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    diff = np.abs(y_pred - y_true) / denominator\n",
    "    diff[denominator == 0] = 0  # avoid division by zero\n",
    "    return np.mean(diff) * 100\n",
    "\n",
    "def smape_eval(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    denominator = (np.abs(y_true) + np.abs(preds)) / 2\n",
    "    diff = np.abs(preds - y_true) / denominator\n",
    "    diff[denominator == 0] = 0\n",
    "    return 'smape', np.mean(diff) * 100, False  # False = lower is better\n",
    "\n",
    "def refined_squared_smape_objective(preds, train_data):\n",
    "    y_true = train_data.get_label()\n",
    "    eps = 1e-6\n",
    "    \n",
    "    abs_y = np.abs(y_true)\n",
    "    abs_pred = np.abs(preds)\n",
    "    denom = (abs_y + abs_pred) / 2 + eps\n",
    "    diff = preds - y_true\n",
    "    \n",
    "    # Refined gradient\n",
    "    grad = 2 * diff / (denom ** 2) - (diff**2) * np.sign(preds) / (denom**3)\n",
    "    \n",
    "    # Refined hessian\n",
    "    hess = 2 / (denom ** 2) + 3 * (diff**2) / (denom ** 4)\n",
    "    \n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "736165bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train, y_train_log)\n",
    "lgb_valid = lgb.Dataset(X_valid, y_valid_log, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'learning_rate': 0.05,\n",
    "    \"metric\": \"mae\",\n",
    "    \"objective\": refined_squared_smape_objective,\n",
    "    'subsample': 0.7,\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    valid_sets=[lgb_valid],\n",
    "    feval=smape_eval,\n",
    "    num_boost_round=5000\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "824857f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(53.31871183377386)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = np.expm1(model.predict(X_valid))\n",
    "final_smape = smape(y_valid, y_pred)\n",
    "final_smape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05fb3929",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_final = np.log1p(df['price'])\n",
    "final_dataset = lgb.Dataset(X_combined, y_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49e1031b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75000,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "515d84c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = lgb.train(\n",
    "    params,\n",
    "    train_set=final_dataset,\n",
    "    num_boost_round=5000,\n",
    "    feval=smape_eval,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7ab8722f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x24085f4f9d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.save_model('lgbm_model.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c7195d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_tfidf.pkl']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# save the fitted vectorizer\n",
    "joblib.dump(tfidf, 'tfidf_vectorizer.pkl')\n",
    "joblib.dump(svd, 'svd_transformer.pkl')\n",
    "joblib.dump(scaler_bert, 'scaler_bert.pkl')\n",
    "joblib.dump(scaler_tfidf, 'scaler_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc0c5810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labelencoder.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(le, \"labelencoder.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9438ee00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
